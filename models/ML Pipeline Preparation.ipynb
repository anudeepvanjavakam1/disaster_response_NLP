{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\avanjavakam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\avanjavakam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\avanjavakam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\avanjavakam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import classification_report, f1_score, recall_score, precision_score, accuracy_score, precision_recall_curve, precision_recall_fscore_support, multilabel_confusion_matrix\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>search_and_rescue</th>\n",
       "      <th>security</th>\n",
       "      <th>military</th>\n",
       "      <th>water</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   related  request  offer  aid_related  medical_help  medical_products  \\\n",
       "0        1        0      0            0             0                 0   \n",
       "1        1        0      0            1             0                 0   \n",
       "2        1        0      0            0             0                 0   \n",
       "3        1        1      0            1             0                 1   \n",
       "4        1        0      0            0             0                 0   \n",
       "\n",
       "   search_and_rescue  security  military  water  ...  aid_centers  \\\n",
       "0                  0         0         0      0  ...            0   \n",
       "1                  0         0         0      0  ...            0   \n",
       "2                  0         0         0      0  ...            0   \n",
       "3                  0         0         0      0  ...            0   \n",
       "4                  0         0         0      0  ...            0   \n",
       "\n",
       "   other_infrastructure  weather_related  floods  storm  fire  earthquake  \\\n",
       "0                     0                0       0      0     0           0   \n",
       "1                     0                1       0      1     0           0   \n",
       "2                     0                0       0      0     0           0   \n",
       "3                     0                0       0      0     0           0   \n",
       "4                     0                0       0      0     0           0   \n",
       "\n",
       "   cold  other_weather  direct_report  \n",
       "0     0              0              0  \n",
       "1     0              0              0  \n",
       "2     0              0              0  \n",
       "3     0              0              0  \n",
       "4     0              0              0  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///disaster_response.db')\n",
    "df = pd.read_sql_table('messages', engine)\n",
    "\n",
    "\n",
    "# Given value 2 in the related field are neglible so it could be error. Replacing 2 with 1 to consider it a valid response.\n",
    "# Alternatively, we could have assumed it to be 0 also. In the absence of information I have gone with majority class.\n",
    "df['related']=df['related'].map(lambda x: 1 if x == 2 else x)\n",
    "\n",
    "#Remove child alone as it has all zeros only\n",
    "df = df.drop(['child_alone'],axis=1)\n",
    "\n",
    "# separate feature and target into X and Y\n",
    "X = df['message']\n",
    "Y = df.drop(['message','id','original','genre'], axis=1)\n",
    "\n",
    "Y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \n",
    "    #removing punctuations, lowering the case and stripping leading and trailing space\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\",\" \", text.lower().strip())\n",
    "    \n",
    "    #tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    #lemmatize initialization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    #lemmatize and store clean tokens\n",
    "    tokens = [lemmatizer.lemmatize(w).lower().strip() for w in tokens]\n",
    "\n",
    "    #return cleaned tokens\n",
    "    return tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline for features, count vectorizer, tdidf transformer and classifier\n",
    "pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "                ('tdidf', TfidfTransformer()),\n",
    "            \n",
    "            ]))\n",
    "\n",
    "        ])),\n",
    "        ('clf', MultiOutputClassifier(estimator = LogisticRegression(random_state=42, verbose = 3)))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 1, 0, 0],\n",
       "       [1, 1, 0, ..., 0, 0, 1],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 1, 0, ..., 0, 0, 1],\n",
       "       [1, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#splitting X, Y into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y)\n",
    "\n",
    "y_test.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 1, 0, ..., 0, 0, 1],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the classifier pipeline on training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# predicting the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.84      0.96      0.90      5014\n",
      "               request       0.82      0.56      0.66      1112\n",
      "                 offer       0.00      0.00      0.00        23\n",
      "           aid_related       0.77      0.66      0.71      2723\n",
      "          medical_help       0.71      0.16      0.26       527\n",
      "      medical_products       0.83      0.21      0.33       326\n",
      "     search_and_rescue       0.83      0.05      0.10       182\n",
      "              security       0.00      0.00      0.00       108\n",
      "              military       0.81      0.12      0.22       208\n",
      "                 water       0.78      0.50      0.61       410\n",
      "                  food       0.85      0.61      0.71       741\n",
      "               shelter       0.83      0.42      0.56       566\n",
      "              clothing       0.65      0.15      0.25        86\n",
      "                 money       0.75      0.06      0.12       144\n",
      "        missing_people       0.00      0.00      0.00        68\n",
      "              refugees       0.61      0.07      0.12       202\n",
      "                 death       0.88      0.22      0.35       311\n",
      "             other_aid       0.65      0.12      0.21       847\n",
      "infrastructure_related       0.64      0.03      0.06       429\n",
      "             transport       0.69      0.09      0.15       282\n",
      "             buildings       0.82      0.22      0.34       347\n",
      "           electricity       0.94      0.12      0.21       130\n",
      "                 tools       0.00      0.00      0.00        36\n",
      "             hospitals       1.00      0.02      0.03        65\n",
      "                 shops       0.00      0.00      0.00        37\n",
      "           aid_centers       0.00      0.00      0.00        75\n",
      "  other_infrastructure       0.60      0.02      0.04       289\n",
      "       weather_related       0.85      0.66      0.75      1835\n",
      "                floods       0.90      0.39      0.54       557\n",
      "                 storm       0.79      0.46      0.58       621\n",
      "                  fire       1.00      0.03      0.05        73\n",
      "            earthquake       0.92      0.64      0.76       626\n",
      "                  cold       0.82      0.14      0.24       127\n",
      "         other_weather       0.56      0.04      0.08       362\n",
      "         direct_report       0.77      0.48      0.59      1274\n",
      "\n",
      "             micro avg       0.82      0.55      0.66     20763\n",
      "             macro avg       0.65      0.23      0.30     20763\n",
      "          weighted avg       0.79      0.55      0.60     20763\n",
      "           samples avg       0.67      0.49      0.52     20763\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# printing precision, recall, f1 score\n",
    "print(classification_report(y_test.values, y_pred, target_names = Y.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for logistic regression classifier\n",
    "parameters = {\n",
    "    'clf__estimator__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'clf__estimator__class_weight': ['balanced',None],\n",
    "    'clf__estimator__solver': ['sag', 'saga']\n",
    "\n",
    "}\n",
    "\n",
    "# using Grid Search with cross-validation on classifier pipeline along with parameters\n",
    "cv = GridSearchCV(pipeline, param_grid = parameters, scoring = 'f1_micro', n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "30 fits failed out of a total of 60.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\pipeline.py\", line 406, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\multioutput.py\", line 451, in fit\n",
      "    super().fit(X, Y, sample_weight, **fit_params)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\multioutput.py\", line 217, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\multioutput.py\", line 50, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\pipeline.py\", line 406, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\multioutput.py\", line 451, in fit\n",
      "    super().fit(X, Y, sample_weight, **fit_params)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\multioutput.py\", line 217, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\multioutput.py\", line 50, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\pipeline.py\", line 406, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\multioutput.py\", line 451, in fit\n",
      "    super().fit(X, Y, sample_weight, **fit_params)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\multioutput.py\", line 217, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\multioutput.py\", line 50, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.50771552 0.49555729 0.37505982        nan        nan\n",
      "        nan 0.6796473  0.64762211 0.64749964        nan        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 47 epochs took 14 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 73 epochs took 10 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    9.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    9.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 7 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 46 epochs took 12 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   11.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   11.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 58 epochs took 6 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 82 epochs took 9 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    8.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    8.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 46 epochs took 5 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    5.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    5.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 9 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    9.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    9.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 77 epochs took 11 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   10.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   10.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 55 epochs took 7 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 55 epochs took 9 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    8.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    8.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 97 epochs took 15 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   15.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   15.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 11 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   11.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   11.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 80 epochs took 8 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 87 epochs took 7 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 13 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 12 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   11.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   11.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 94 epochs took 14 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   14.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   14.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 74 epochs took 7 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 59 epochs took 4 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 63 epochs took 4 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 5 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 5 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    5.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    5.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 99 epochs took 5 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 4 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 6 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    5.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    5.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 88 epochs took 5 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    5.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    5.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 54 epochs took 6 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    5.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    5.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 54 epochs took 3 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 67 epochs took 5 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    5.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    5.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 76 epochs took 3 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 6 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 69 epochs took 3 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 7 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 76 epochs took 8 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.8s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=Pipeline(steps=[(&#x27;features&#x27;,\n",
       "                                        FeatureUnion(transformer_list=[(&#x27;text_pipeline&#x27;,\n",
       "                                                                        Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                                                                                         CountVectorizer(tokenizer=&lt;function tokenize at 0x0000029F689EF880&gt;)),\n",
       "                                                                                        (&#x27;tdidf&#x27;,\n",
       "                                                                                         TfidfTransformer())]))])),\n",
       "                                       (&#x27;clf&#x27;,\n",
       "                                        MultiOutputClassifier(estimator=LogisticRegression(random_state=42,\n",
       "                                                                                           verbose=3)))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;clf__estimator__class_weight&#x27;: [&#x27;balanced&#x27;, None],\n",
       "                         &#x27;clf__estimator__penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;elasticnet&#x27;],\n",
       "                         &#x27;clf__estimator__solver&#x27;: [&#x27;sag&#x27;, &#x27;saga&#x27;]},\n",
       "             scoring=&#x27;f1_micro&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=Pipeline(steps=[(&#x27;features&#x27;,\n",
       "                                        FeatureUnion(transformer_list=[(&#x27;text_pipeline&#x27;,\n",
       "                                                                        Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                                                                                         CountVectorizer(tokenizer=&lt;function tokenize at 0x0000029F689EF880&gt;)),\n",
       "                                                                                        (&#x27;tdidf&#x27;,\n",
       "                                                                                         TfidfTransformer())]))])),\n",
       "                                       (&#x27;clf&#x27;,\n",
       "                                        MultiOutputClassifier(estimator=LogisticRegression(random_state=42,\n",
       "                                                                                           verbose=3)))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;clf__estimator__class_weight&#x27;: [&#x27;balanced&#x27;, None],\n",
       "                         &#x27;clf__estimator__penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;elasticnet&#x27;],\n",
       "                         &#x27;clf__estimator__solver&#x27;: [&#x27;sag&#x27;, &#x27;saga&#x27;]},\n",
       "             scoring=&#x27;f1_micro&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;features&#x27;,\n",
       "                 FeatureUnion(transformer_list=[(&#x27;text_pipeline&#x27;,\n",
       "                                                 Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                                                                  CountVectorizer(tokenizer=&lt;function tokenize at 0x0000029F689EF880&gt;)),\n",
       "                                                                 (&#x27;tdidf&#x27;,\n",
       "                                                                  TfidfTransformer())]))])),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 MultiOutputClassifier(estimator=LogisticRegression(random_state=42,\n",
       "                                                                    verbose=3)))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">features: FeatureUnion</label><div class=\"sk-toggleable__content\"><pre>FeatureUnion(transformer_list=[(&#x27;text_pipeline&#x27;,\n",
       "                                Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                                                 CountVectorizer(tokenizer=&lt;function tokenize at 0x0000029F689EF880&gt;)),\n",
       "                                                (&#x27;tdidf&#x27;,\n",
       "                                                 TfidfTransformer())]))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>text_pipeline</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(tokenizer=&lt;function tokenize at 0x0000029F689EF880&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">clf: MultiOutputClassifier</label><div class=\"sk-toggleable__content\"><pre>MultiOutputClassifier(estimator=LogisticRegression(random_state=42, verbose=3))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=42, verbose=3)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=42, verbose=3)</pre></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('features',\n",
       "                                        FeatureUnion(transformer_list=[('text_pipeline',\n",
       "                                                                        Pipeline(steps=[('vect',\n",
       "                                                                                         CountVectorizer(tokenizer=<function tokenize at 0x0000029F689EF880>)),\n",
       "                                                                                        ('tdidf',\n",
       "                                                                                         TfidfTransformer())]))])),\n",
       "                                       ('clf',\n",
       "                                        MultiOutputClassifier(estimator=LogisticRegression(random_state=42,\n",
       "                                                                                           verbose=3)))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'clf__estimator__class_weight': ['balanced', None],\n",
       "                         'clf__estimator__penalty': ['l1', 'l2', 'elasticnet'],\n",
       "                         'clf__estimator__solver': ['sag', 'saga']},\n",
       "             scoring='f1_micro')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the pipeline for best parameters\n",
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model with best parameters\n",
    "y_pred = cv.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.85      0.95      0.90      5014\n",
      "               request       0.80      0.57      0.66      1138\n",
      "                 offer       0.00      0.00      0.00        26\n",
      "           aid_related       0.78      0.68      0.72      2722\n",
      "          medical_help       0.63      0.26      0.36       527\n",
      "      medical_products       0.69      0.30      0.42       335\n",
      "     search_and_rescue       0.79      0.19      0.30       180\n",
      "              security       1.00      0.01      0.02       112\n",
      "              military       0.68      0.34      0.45       214\n",
      "                 water       0.80      0.63      0.71       427\n",
      "                  food       0.83      0.68      0.75       739\n",
      "               shelter       0.79      0.57      0.66       568\n",
      "              clothing       0.72      0.47      0.57       104\n",
      "                 money       0.62      0.25      0.36       130\n",
      "        missing_people       0.65      0.14      0.23        79\n",
      "              refugees       0.70      0.24      0.36       236\n",
      "                 death       0.77      0.50      0.60       286\n",
      "             other_aid       0.56      0.16      0.25       836\n",
      "infrastructure_related       0.48      0.07      0.12       405\n",
      "             transport       0.76      0.21      0.33       311\n",
      "             buildings       0.75      0.33      0.46       330\n",
      "           electricity       0.68      0.28      0.39       138\n",
      "                 tools       0.00      0.00      0.00        43\n",
      "             hospitals       0.45      0.09      0.15        54\n",
      "                 shops       0.00      0.00      0.00        25\n",
      "           aid_centers       0.40      0.03      0.05        73\n",
      "  other_infrastructure       0.38      0.05      0.08       284\n",
      "       weather_related       0.86      0.71      0.78      1775\n",
      "                floods       0.89      0.54      0.67       524\n",
      "                 storm       0.74      0.57      0.64       567\n",
      "                  fire       0.59      0.26      0.36        73\n",
      "            earthquake       0.90      0.77      0.83       619\n",
      "                  cold       0.68      0.28      0.40       135\n",
      "         other_weather       0.54      0.10      0.18       325\n",
      "         direct_report       0.74      0.52      0.61      1259\n",
      "\n",
      "             micro avg       0.81      0.60      0.69     20613\n",
      "             macro avg       0.64      0.34      0.41     20613\n",
      "          weighted avg       0.77      0.60      0.65     20613\n",
      "           samples avg       0.65      0.52      0.53     20613\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# printing precision, recall and f1 score\n",
    "print(classification_report(y_test.values, y_pred, target_names = Y.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StartingVerbExtractor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StartingVerbExtractor</label><div class=\"sk-toggleable__content\"><pre>StartingVerbExtractor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StartingVerbExtractor()"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building a custom transformer to extract the starting verb of a sentence\n",
    "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Starting Verb Extractor class\n",
    "    \n",
    "    This class extract the starting verb of the message,\n",
    "    creating a new feature for the ML classifier\n",
    "    \"\"\"\n",
    "    def starting_verb(self, text):\n",
    "        \n",
    "        # sentence tokenizing the text\n",
    "        sentence_list = nltk.sent_tokenize(text)\n",
    "        \n",
    "        \n",
    "        for sentence in sentence_list:\n",
    "            # tokenize each sentence and tag parts of speech\n",
    "            pos_tags = nltk.pos_tag(tokenize(sentence))\n",
    "            \n",
    "            try:\n",
    "                # extract the first word and its tagged part of speech\n",
    "                first_word, first_tag = pos_tags[0]\n",
    "                #if it's a verb, return true\n",
    "                if first_tag in ['VB','VBP'] or first_word == 'RT':\n",
    "                    return float(1)\n",
    "            except:\n",
    "                # In all other cases, return false\n",
    "                return float(0)\n",
    "\n",
    "    #Given it is a transformer, return self\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    # defining transform\n",
    "    def transform(self, X):\n",
    "        # applying the starting_verb function to add this feature\n",
    "        X_with_starting_verb = pd.Series(X).apply(self.starting_verb)\n",
    "        return pd.DataFrame(X_with_starting_verb).dropna(inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improving our existing Pipeline with added feature - StartingVerbExtractor and an ensemble classifier\n",
    "pipeline_starting_verb_ext = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "\n",
    "            ('starting_verb_transformer', StartingVerbExtractor())\n",
    "        ])),\n",
    "\n",
    "        ('clf', MultiOutputClassifier(AdaBoostClassifier()))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 1, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#splitting data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y)\n",
    "\n",
    "#fitting new pipeline with added feature\n",
    "pipeline_starting_verb_ext.fit(X_train, y_train)\n",
    "\n",
    "#predicting\n",
    "y_pred = pipeline_starting_verb_ext.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.83      0.95      0.88      5021\n",
      "               request       0.76      0.53      0.62      1126\n",
      "                 offer       0.00      0.00      0.00        32\n",
      "           aid_related       0.76      0.58      0.66      2760\n",
      "          medical_help       0.63      0.28      0.39       524\n",
      "      medical_products       0.66      0.32      0.43       332\n",
      "     search_and_rescue       0.60      0.16      0.26       179\n",
      "              security       0.33      0.05      0.08       128\n",
      "              military       0.58      0.29      0.38       221\n",
      "                 water       0.73      0.65      0.69       429\n",
      "                  food       0.82      0.66      0.73       705\n",
      "               shelter       0.77      0.52      0.62       578\n",
      "              clothing       0.75      0.44      0.55       109\n",
      "                 money       0.56      0.30      0.40       158\n",
      "        missing_people       0.60      0.20      0.30        61\n",
      "              refugees       0.55      0.27      0.36       224\n",
      "                 death       0.73      0.48      0.58       297\n",
      "             other_aid       0.51      0.16      0.24       873\n",
      "infrastructure_related       0.42      0.10      0.16       430\n",
      "             transport       0.64      0.27      0.38       274\n",
      "             buildings       0.62      0.36      0.46       297\n",
      "           electricity       0.53      0.26      0.35       130\n",
      "                 tools       0.22      0.05      0.09        37\n",
      "             hospitals       0.39      0.11      0.17        64\n",
      "                 shops       0.00      0.00      0.00        36\n",
      "           aid_centers       0.47      0.12      0.19        78\n",
      "  other_infrastructure       0.46      0.07      0.13       294\n",
      "       weather_related       0.85      0.67      0.75      1843\n",
      "                floods       0.82      0.58      0.68       526\n",
      "                 storm       0.73      0.51      0.60       623\n",
      "                  fire       0.46      0.18      0.26        65\n",
      "            earthquake       0.89      0.75      0.81       627\n",
      "                  cold       0.83      0.35      0.49       151\n",
      "         other_weather       0.50      0.16      0.25       349\n",
      "         direct_report       0.74      0.46      0.57      1308\n",
      "\n",
      "             micro avg       0.78      0.58      0.66     20889\n",
      "             macro avg       0.59      0.34      0.41     20889\n",
      "          weighted avg       0.74      0.58      0.63     20889\n",
      "           samples avg       0.65      0.51      0.52     20889\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#reporting classification metrics\n",
    "print(classification_report(y_test.values, y_pred, target_names = Y.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=Pipeline(steps=[(&#x27;features&#x27;,\n",
       "                                        FeatureUnion(transformer_list=[(&#x27;text_pipeline&#x27;,\n",
       "                                                                        Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                                                                                         CountVectorizer(tokenizer=&lt;function tokenize at 0x0000029F037EEB00&gt;)),\n",
       "                                                                                        (&#x27;tfidf&#x27;,\n",
       "                                                                                         TfidfTransformer())])),\n",
       "                                                                       (&#x27;starting_verb_transformer&#x27;,\n",
       "                                                                        StartingVerbExtractor())])),\n",
       "                                       (&#x27;clf&#x27;,\n",
       "                                        MultiOutputClassifier(estimator=AdaBoostClassifier()))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;clf__estimator__learning_rate&#x27;: [0.001, 0.1, 1],\n",
       "                         &#x27;clf__estimator__n_estimators&#x27;: [10, 50, 100]},\n",
       "             scoring=&#x27;f1_micro&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=Pipeline(steps=[(&#x27;features&#x27;,\n",
       "                                        FeatureUnion(transformer_list=[(&#x27;text_pipeline&#x27;,\n",
       "                                                                        Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                                                                                         CountVectorizer(tokenizer=&lt;function tokenize at 0x0000029F037EEB00&gt;)),\n",
       "                                                                                        (&#x27;tfidf&#x27;,\n",
       "                                                                                         TfidfTransformer())])),\n",
       "                                                                       (&#x27;starting_verb_transformer&#x27;,\n",
       "                                                                        StartingVerbExtractor())])),\n",
       "                                       (&#x27;clf&#x27;,\n",
       "                                        MultiOutputClassifier(estimator=AdaBoostClassifier()))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;clf__estimator__learning_rate&#x27;: [0.001, 0.1, 1],\n",
       "                         &#x27;clf__estimator__n_estimators&#x27;: [10, 50, 100]},\n",
       "             scoring=&#x27;f1_micro&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;features&#x27;,\n",
       "                 FeatureUnion(transformer_list=[(&#x27;text_pipeline&#x27;,\n",
       "                                                 Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                                                                  CountVectorizer(tokenizer=&lt;function tokenize at 0x0000029F037EEB00&gt;)),\n",
       "                                                                 (&#x27;tfidf&#x27;,\n",
       "                                                                  TfidfTransformer())])),\n",
       "                                                (&#x27;starting_verb_transformer&#x27;,\n",
       "                                                 StartingVerbExtractor())])),\n",
       "                (&#x27;clf&#x27;, MultiOutputClassifier(estimator=AdaBoostClassifier()))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">features: FeatureUnion</label><div class=\"sk-toggleable__content\"><pre>FeatureUnion(transformer_list=[(&#x27;text_pipeline&#x27;,\n",
       "                                Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                                                 CountVectorizer(tokenizer=&lt;function tokenize at 0x0000029F037EEB00&gt;)),\n",
       "                                                (&#x27;tfidf&#x27;,\n",
       "                                                 TfidfTransformer())])),\n",
       "                               (&#x27;starting_verb_transformer&#x27;,\n",
       "                                StartingVerbExtractor())])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>text_pipeline</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(tokenizer=&lt;function tokenize at 0x0000029F037EEB00&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>starting_verb_transformer</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StartingVerbExtractor</label><div class=\"sk-toggleable__content\"><pre>StartingVerbExtractor()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">clf: MultiOutputClassifier</label><div class=\"sk-toggleable__content\"><pre>MultiOutputClassifier(estimator=AdaBoostClassifier())</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: AdaBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>AdaBoostClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>AdaBoostClassifier()</pre></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('features',\n",
       "                                        FeatureUnion(transformer_list=[('text_pipeline',\n",
       "                                                                        Pipeline(steps=[('vect',\n",
       "                                                                                         CountVectorizer(tokenizer=<function tokenize at 0x0000029F037EEB00>)),\n",
       "                                                                                        ('tfidf',\n",
       "                                                                                         TfidfTransformer())])),\n",
       "                                                                       ('starting_verb_transformer',\n",
       "                                                                        StartingVerbExtractor())])),\n",
       "                                       ('clf',\n",
       "                                        MultiOutputClassifier(estimator=AdaBoostClassifier()))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'clf__estimator__learning_rate': [0.001, 0.1, 1],\n",
       "                         'clf__estimator__n_estimators': [10, 50, 100]},\n",
       "             scoring='f1_micro')"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameters for adaboost classifier\n",
    "parameters = {\n",
    "    'clf__estimator__n_estimators': [10, 50, 100],\n",
    "    'clf__estimator__learning_rate': [0.001, 0.1, 1]\n",
    "}\n",
    "\n",
    "# passing new pipeline and parameters for grid search\n",
    "cv = GridSearchCV(pipeline_starting_verb_ext, param_grid = parameters, scoring = 'f1_micro', n_jobs = -1)\n",
    "\n",
    "#fitting the new pipeline for best parameters\n",
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__learning_rate': 1, 'clf__estimator__n_estimators': 100}"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicting with best parameters\n",
    "y_pred = cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.84      0.94      0.89      5021\n",
      "               request       0.76      0.56      0.64      1126\n",
      "                 offer       0.00      0.00      0.00        32\n",
      "           aid_related       0.77      0.61      0.68      2760\n",
      "          medical_help       0.61      0.27      0.38       524\n",
      "      medical_products       0.61      0.33      0.42       332\n",
      "     search_and_rescue       0.47      0.17      0.25       179\n",
      "              security       0.38      0.09      0.14       128\n",
      "              military       0.60      0.33      0.42       221\n",
      "                 water       0.72      0.64      0.68       429\n",
      "                  food       0.81      0.64      0.72       705\n",
      "               shelter       0.75      0.53      0.62       578\n",
      "              clothing       0.73      0.45      0.56       109\n",
      "                 money       0.49      0.25      0.33       158\n",
      "        missing_people       0.54      0.25      0.34        61\n",
      "              refugees       0.52      0.28      0.36       224\n",
      "                 death       0.76      0.47      0.59       297\n",
      "             other_aid       0.50      0.19      0.28       873\n",
      "infrastructure_related       0.39      0.10      0.16       430\n",
      "             transport       0.63      0.24      0.35       274\n",
      "             buildings       0.62      0.40      0.49       297\n",
      "           electricity       0.55      0.28      0.37       130\n",
      "                 tools       0.10      0.03      0.04        37\n",
      "             hospitals       0.32      0.14      0.20        64\n",
      "                 shops       0.00      0.00      0.00        36\n",
      "           aid_centers       0.29      0.13      0.18        78\n",
      "  other_infrastructure       0.40      0.09      0.14       294\n",
      "       weather_related       0.85      0.69      0.76      1843\n",
      "                floods       0.79      0.57      0.66       526\n",
      "                 storm       0.70      0.52      0.60       623\n",
      "                  fire       0.41      0.14      0.21        65\n",
      "            earthquake       0.87      0.73      0.80       627\n",
      "                  cold       0.73      0.37      0.49       151\n",
      "         other_weather       0.47      0.17      0.25       349\n",
      "         direct_report       0.73      0.48      0.58      1308\n",
      "\n",
      "             micro avg       0.77      0.59      0.67     20889\n",
      "             macro avg       0.56      0.35      0.42     20889\n",
      "          weighted avg       0.73      0.59      0.64     20889\n",
      "           samples avg       0.63      0.51      0.52     20889\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\avanjavakam\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# printing precision, recall, f1 score\n",
    "print(classification_report(y_test.values, y_pred, target_names = Y.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cv, open('disaster_response_cv_classifier.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sqlite:///sss'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('datascience')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "dfe8d95a8124d40da220b1452b62fe5b24aa21558a1809141d040387d1655c44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
